{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eee111d5",
   "metadata": {},
   "source": [
    "#### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "abd8b382",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-12T05:13:08.760523Z",
     "start_time": "2021-12-12T05:13:07.544122Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import nltk\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, HistGradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV, ParameterGrid\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2f553e88",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-12T05:13:08.760523Z",
     "start_time": "2021-12-12T05:13:07.544122Z"
    }
   },
   "outputs": [],
   "source": [
    "#LIAR = pd.read_csv(\"/content/sample_data/LIAR.csv\", index_col=0)\n",
    "df = pd.read_csv(\"LIAR.csv\", index_col=0)\n",
    "columns = [\"id\",\n",
    "           \"label\",\n",
    "           \"statement\",\n",
    "           \"subject\",\n",
    "           \"speaker\",\n",
    "           \"job_title\",\n",
    "           \"state_info\",\n",
    "           \"party_affiliation\",\n",
    "           \"barely_true_counts\",\n",
    "           \"false_counts\",\n",
    "           \"half_true_counts\",\n",
    "           \"mostly_true_counts\",\n",
    "           \"pants_on_fire_counts\",\n",
    "           \"context\",\n",
    "           \"justification\"]\n",
    "df.columns = columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70f842d",
   "metadata": {},
   "source": [
    "#### Deal with Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "64ebd521",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     12565\n",
       "unique       83\n",
       "top       Texas\n",
       "freq       1568\n",
       "Name: state_info, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define columns with missing values\n",
    "columns_with_missing = ['subject', 'party_affiliation', 'barely_true_counts', 'false_counts', 'half_true_counts', \n",
    "                        'mostly_true_counts', 'pants_on_fire_counts', 'context', 'justification']\n",
    "\n",
    "# Drop rows with missing values in specified columns\n",
    "df.dropna(subset=columns_with_missing, inplace=True)\n",
    "\n",
    "# Fill missing values in 'job_title' column\n",
    "df['job_title'].fillna(pd.Series(np.random.choice(df['job_title'].dropna(), size=len(df))), inplace=True)\n",
    "\n",
    "# Fill missing values in 'state_info' column\n",
    "df['state_info'].fillna(pd.Series(np.random.choice(df['state_info'].dropna(), size=len(df))), inplace=True)\n",
    "df.state_info.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "19ed9962",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count         12565\n",
       "unique         1343\n",
       "top       President\n",
       "freq            822\n",
       "Name: job_title, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.job_title.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f8088b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-12T02:55:19.128489Z",
     "start_time": "2021-12-12T02:55:19.124927Z"
    }
   },
   "source": [
    "#### Map to binary outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9dce6b28",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-12T05:13:08.920387Z",
     "start_time": "2021-12-12T05:13:08.800610Z"
    }
   },
   "outputs": [],
   "source": [
    "def mapping_label(s):\n",
    "    label_mapping = {\"pants-fire\": 0, \"false\": 0, \"barely-true\": 0, \"half-true\": 0, \"mostly-true\": 1, \"true\": 1}\n",
    "    return label_mapping.get(s.lower(), -1)\n",
    "\n",
    "df['integer_label'] = df.label.apply(mapping_label)\n",
    "all_subjects = df.subject.str.split(',',expand = True)\n",
    "all_subjects = pd.DataFrame(np.array(all_subjects.iloc[:,:4]),columns = ['subject1','subject2','subject3','subject4'])\n",
    "df = pd.concat([df.reset_index(),all_subjects.reset_index()],axis=1)  #added reset_index to avoid incresing row #\n",
    "df = df.drop(['label','subject','index'],axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8329d3",
   "metadata": {},
   "source": [
    "#### Encoding Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "10762186",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-12T05:13:09.116350Z",
     "start_time": "2021-12-12T05:13:08.947894Z"
    }
   },
   "outputs": [],
   "source": [
    "def top_n_encoder(series, n,prefix = None):\n",
    "    '''\n",
    "    series: a pd series\n",
    "    n: number of top frequent categories to be encoded\n",
    "    '''\n",
    "    series = series.copy()\n",
    "    counts = series.value_counts()\n",
    "    mask = series.isin(counts.iloc[:n].index)\n",
    "    series[~mask] = \"other\"\n",
    "    return pd.get_dummies(series,prefix = prefix)\n",
    "\n",
    "encoded_job_title = top_n_encoder(df.job_title.str.lower(),n = 100,prefix = \"job\")\n",
    "encoded_party = top_n_encoder(df.party_affiliation.str.lower(),n = 100,prefix = \"party\")\n",
    "encoded_speaker = top_n_encoder(df.speaker.str.lower(),n = 100,prefix = \"speaker\")\n",
    "encoded_state_info = top_n_encoder(df.state_info.str.lower(),n = 100,prefix = \"state_info\") #added this line\n",
    "encoded_context = top_n_encoder(df.context.str.lower(), n = 150,prefix = \"context\")\n",
    "encoded_subject1 = top_n_encoder(df.subject1.str.lower(),n = 150, prefix = \"subject1\")\n",
    "encoded_subject2 = top_n_encoder(df.subject2.str.lower(),n = 150,prefix = \"subject2\")\n",
    "encoded_subject3 = top_n_encoder(df.subject3.str.lower(),n = 150,prefix = \"subject3\")\n",
    "encoded_subject4 = top_n_encoder(df.subject4.str.lower(),n = 150,prefix = \"subject4\")\n",
    "\n",
    "categorical_features = pd.concat([encoded_job_title,encoded_party,encoded_speaker,encoded_context,\n",
    "                                  encoded_subject1,encoded_subject2,encoded_subject3,encoded_subject4],axis = 1)\n",
    "df = pd.concat([df.reset_index(),categorical_features.reset_index()],axis=1) # concat encoded cols\n",
    "df = df.drop(columns=['job_title','party_affiliation','speaker','state_info','context','index',\n",
    "                      'subject1','subject2','subject3','subject4']) # drop oringinal cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9ff01b",
   "metadata": {},
   "source": [
    "#### Cleaning the statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8d388877",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-12T05:13:11.910029Z",
     "start_time": "2021-12-12T05:13:09.118245Z"
    }
   },
   "outputs": [],
   "source": [
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = BeautifulSoup(text, \"lxml\").text # HTML decoding\n",
    "    text = text.lower() # lowercase text\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
    "    text = BAD_SYMBOLS_RE.sub('', text) # delete symbols which are in BAD_SYMBOLS_RE from text\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # delete stopwors from text\n",
    "    return text\n",
    "\n",
    "df['statement_clean'] = df.statement.apply(clean_text)\n",
    "df = df.drop(columns=['statement'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eacf39a",
   "metadata": {},
   "source": [
    "#### split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1479f310",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-12T05:13:12.040888Z",
     "start_time": "2021-12-12T05:13:11.912096Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = df.drop(columns=['integer_label','id'])\n",
    "y = df['integer_label']\n",
    "X_dev, X_test, y_dev, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_dev, y_dev, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28532bdc",
   "metadata": {},
   "source": [
    "#### tokenize the statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7b44220b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-12T05:13:12.211447Z",
     "start_time": "2021-12-12T05:13:12.044336Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Convert a collection of text documents to a matrix of token counts\n",
    "vectorizer = CountVectorizer() \n",
    "X_text_train = vectorizer.fit_transform(X_train.statement_clean)\n",
    "X_text_val = vectorizer.transform(X_val.statement_clean)\n",
    "X_text_test = vectorizer.transform(X_test.statement_clean)\n",
    "\n",
    "#Transform a count matrix to a normalized tf or tf-idf representation\n",
    "tfidf = TfidfTransformer() \n",
    "X_text_train = tfidf.fit_transform(X_text_train)\n",
    "X_text_val = tfidf.transform(X_text_val)\n",
    "X_text_test = tfidf.transform(X_text_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff00bf2",
   "metadata": {},
   "source": [
    "In X, the original 'statement' is cleaned and stored in column 'statement_clean', <br>\n",
    "all other columsn are numerical including those encoded from categorical variables.<br>\n",
    "'statement_clean' variable is further tokenized and splitted into X_text_train, X_text_val, X_text_test.\n",
    "#### Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ded0eb8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-12T05:13:12.276473Z",
     "start_time": "2021-12-12T05:13:12.213334Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'X_text_train' (csr_matrix)\n",
      "Stored 'X_text_val' (csr_matrix)\n",
      "Stored 'X_text_test' (csr_matrix)\n",
      "Stored 'X_train' (DataFrame)\n",
      "Stored 'X_val' (DataFrame)\n",
      "Stored 'X_test' (DataFrame)\n",
      "Stored 'y_train' (Series)\n",
      "Stored 'y_val' (Series)\n",
      "Stored 'y_test' (Series)\n"
     ]
    }
   ],
   "source": [
    "# use % store -r [variable_name] to access the variables\n",
    "%store X_text_train\n",
    "%store X_text_val\n",
    "%store X_text_test\n",
    "%store X_train\n",
    "%store X_val\n",
    "%store X_test\n",
    "%store y_train\n",
    "%store y_val\n",
    "%store y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9fd60f",
   "metadata": {},
   "source": [
    "### Baseline Model: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e1dc0a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.8104265402843602\n",
      "val score: 0.6022906793048973\n",
      "test score: 0.6078199052132701\n"
     ]
    }
   ],
   "source": [
    "logr = LogisticRegression()\n",
    "logr.fit(X_text_train, y_train)\n",
    "\n",
    "logr_pred_train = logr.predict(X_text_train)\n",
    "logr_pred_val = logr.predict(X_text_val)\n",
    "logr_pred_test = logr.predict(X_text_test)\n",
    "\n",
    "print(\"train score:\", logr.score(X_text_train, y_train))\n",
    "print(\"val score:\", logr.score(X_text_val, y_val))\n",
    "print(\"test score:\", logr.score(X_text_test, y_test))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e27e9aa",
   "metadata": {},
   "source": [
    "### Baseline Model: Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6c064488",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.9636650868878357\n",
      "val score: 0.6113744075829384\n",
      "test score: 0.6216429699842022\n"
     ]
    }
   ],
   "source": [
    "clf = svm.SVC()\n",
    "clf.fit(X_text_train, y_train)\n",
    "\n",
    "pred_train = clf.predict(X_text_train)\n",
    "pred_val = clf.predict(X_text_val)\n",
    "pred_test = clf.predict(X_text_test)\n",
    "\n",
    "print(\"train score:\", clf.score(X_text_train, y_train))\n",
    "print(\"val score:\", clf.score(X_text_val, y_val))\n",
    "print(\"test score:\", clf.score(X_text_test, y_test))     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "31f81a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test performance: 0.6097946287519748\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(random_state=84)\n",
    "rf.fit(X_text_train, y_train)\n",
    "\n",
    "print(\"test performance:\", rf.score(X_text_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "eaa1fad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.42      0.48      1113\n",
      "           1       0.62      0.76      0.68      1419\n",
      "\n",
      "    accuracy                           0.61      2532\n",
      "   macro avg       0.60      0.59      0.58      2532\n",
      "weighted avg       0.60      0.61      0.60      2532\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.38      0.47      1113\n",
      "           1       0.62      0.81      0.71      1419\n",
      "\n",
      "    accuracy                           0.62      2532\n",
      "   macro avg       0.62      0.60      0.59      2532\n",
      "weighted avg       0.62      0.62      0.60      2532\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "logr_cv = GridSearchCV(logr,{'alpha':('linear', 'rbf'), 'C':np.logspace(-3,3,20)})\n",
    "\n",
    "svm_cv.fit(X_text_train, y_train)\n",
    "\n",
    "print(classification_report(y_test, logr_pred_test))\n",
    "print(classification_report(y_test, pred_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
