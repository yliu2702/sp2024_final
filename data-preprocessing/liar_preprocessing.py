# -*- coding: utf-8 -*-
"""LIAR_preprocessing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TYdNujQ7taZXmiSG0ZKmizjEyA4zSNki
"""
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import re
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
from base.constant import DATA_BASE_DIR

# print(DATA_BASE_DIR)
# exit()

# CACHE_DIR = os.environ.get("TRANSFORMERS_CACHE")
# DATA_BASE_DIR = "/projectnb/ds598/projects/yliu2702/data"

data = pd.read_csv(DATA_BASE_DIR + "/LIAR.csv",index_col = 0)

"""### Dataframe preprocessing:
1. change column names
2. drop rows with nan rows; give random values to rows with more nan
3. preprocess categorical features, like labels(Y); subjects(split into 4 columns); transform 'speaker', 'job_title', 'state_info','party_affiliation','context' and integrated 'subject 1-4' into dummy variable. Only consider index whose value counts exceed certain threshold
4. Clean the statement and justification (text variable)
"""

columns = ["id",
           "label",
           "statement",
           "subject",
           "speaker",
           "job_title",
           "state_info",
           "party_affiliation",
           "barely_true_counts",
           "false_counts",
           "half_true_counts",
           "mostly_true_counts",
           "pants_on_fire_counts",
           "context",
           "justification"]
data.columns = columns

to_drop = ['subject', 'party_affiliation', 'barely_true_counts', 'false_counts', 'half_true_counts', 'mostly_true_counts'
            , 'pants_on_fire_counts', 'context', 'justification']
df = data.dropna(subset=to_drop)
# give a random job_title, state_info
job_title_probs = df['job_title'].value_counts(normalize=True)
df.loc[df['job_title'].isna(), 'job_title'] = np.random.choice(job_title_probs.index,
                                                               size=df['job_title'].isna().sum(),
                                                               p=job_title_probs.values)

state_info_probs = df['state_info'].value_counts(normalize=True)
df.loc[df['state_info'].isna(), 'state_info'] = np.random.choice(state_info_probs.index,
                                                                 size=df['state_info'].isna().sum(),
                                                                 p=state_info_probs.values)

def mapping_label(s):
    if s == "pants-fire":
        return 0
    elif s == "FALSE":
        return 1
    elif s == "barely-true":
        return 2
    elif s == "half-true":
        return 3
    elif s == "mostly-true":
        return 4
    elif s == "TRUE":
        return 5
    else:
        return -1
df['label_num'] = df.label.apply(mapping_label)
def mapping_label_tf(s):
    if s == "pants-fire":
        return 0
    elif s == "FALSE":
        return 0
    elif s == "barely-true":
        return 0
    elif s == "half-true":
        return 1
    elif s == "mostly-true":
        return 1
    elif s == "TRUE":
        return 1
    else:
        return -1
df['label_tf'] = df.label.apply(mapping_label_tf)

all_subjects = df.subject.str.split(',',expand = True)
all_subjects = pd.DataFrame(np.array(all_subjects.iloc[:,:4]),columns = ['subject1','subject2','subject3','subject4'])
df = pd.concat([df.reset_index(),all_subjects.reset_index()],axis=1)  #added reset_index to avoid incresing row #
df = df.drop(['label','subject','index'],axis = 1)


def top_n_encoder(series, threshold, prefix=None):
    '''
    series: a pd series
    threshold: the minimum frequency threshold for categories to be included
    '''
    series = series.copy()
    counts = series.value_counts(normalize=True)
    mask = counts > threshold
    selected_categories = counts[mask].index
    series[~series.isin(selected_categories)] = "other"
    return pd.get_dummies(series, prefix=prefix)

# 'speaker', 'job_title', 'state_info', 'party_affiliation','context'
encoded_job_title = top_n_encoder(df['job_title'].str.lower(), 0.035, prefix='job')
# title_retained = (encoded_job_title != 0).sum(axis = 0).sum()
# print(encoded_job_title.shape)
encoded_speaker = top_n_encoder(df['speaker'].str.lower(), 0.015, prefix='speaker')
# print(encoded_speaker.shape)
encoded_state = top_n_encoder(df['state_info'].str.lower(), 0.06, prefix='state')
# print(encoded_state.shape)
encoded_party = top_n_encoder(df['party_affiliation'].str.lower(), 0.01, prefix='party')
# print(encoded_party.shape)
encoded_context = top_n_encoder(df['context'].str.lower(), 0.015, prefix='context')
# print(encoded_context.shape)

categorical_features = pd.concat([encoded_job_title,encoded_party,encoded_speaker,encoded_context],axis = 1)
df = pd.concat([df.reset_index(),categorical_features.reset_index()],axis=1) # concat encoded cols
df = df.drop(columns=['job_title','party_affiliation','speaker','state_info','context','index']) # drop oringinal cols

df_copy = df.copy()

total_counts = df[['subject1','subject2', 'subject3', 'subject4']].stack().value_counts(normalize=True)
selected_subject = total_counts[total_counts > 0.01].index
for sub in selected_subject:
    df_copy[f"subject_{sub}"] = df_copy[['subject1','subject2', 'subject3', 'subject4']].apply(lambda row: 1 if sub in row.values else 0, axis=1)
# check new added dummy variable
# [x for x in df_copy.columns if x not in df.columns]

df_copy = df_copy.drop(columns = ['subject1', 'subject2','subject3', 'subject4'])

REPLACE_BY_SPACE_RE = re.compile('[/(){}\[\]\|@,;]')
BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')
STOPWORDS = set(stopwords.words('english'))

def clean_text(text):
    text = text.lower() # lowercase text
    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text
    text = BAD_SYMBOLS_RE.sub('', text) # delete symbols which are in BAD_SYMBOLS_RE from text
    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # delete stopwors from text
    return text

df_copy['statement_clean'] = df_copy.statement.apply(clean_text)
df_copy = df_copy.drop(columns=['statement'])
df_copy['justification_clean'] = df_copy.justification.apply(clean_text)
df_copy = df_copy.drop(columns=['justification'])

df_copy.to_csv(DATA_BASE_DIR + "/LIAR_all_feature.csv")

df_copy.loc[:,['statement_clean', 'justification_clean','label_num', 'label_tf']].to_csv(DATA_BASE_DIR + "/LIAR_text_label.csv")